{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f32a9bb2-96d1-4ce0-8b6b-3aec783797bb",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #ccffcc; padding: 10px;\">\n",
    "    <h1> Tutorial 2 </h1>\n",
    "    <h2> Introduction to Modal Decomposition </h2>\n",
    "</div>\n",
    "\n",
    "# Overview\n",
    "\n",
    "This Jupyter notebook demonstrates how modal decomposition methods can be used for flow feature extraction in fluid mechanics datasets. Modal decomposition techniques, such as Proper Orthogonal Decomposition (POD) and Dynamic Mode Decomposition (DMD), help identify coherent structures in fluid flows, providing a useful dimension reduction for subsequent reduced order modelling. The example application focuses on the classic problem of fluid flow past a cylinder, showing how these methods can simplify complex flow fields into a manageable number of modes. This dimension reduction enables efficient and accurate reduced order modelling using Sparse Identification of Nonlinear Dynamics (SINDy).\n",
    "\n",
    "## Recommended reading\n",
    "\n",
    "* [Modal Analysis of Fluid Flows: An Overview](https://doi.org/10.2514/1.J056060)\n",
    "* [Reduced order modelling](https://uk.mathworks.com/discovery/reduced-order-modeling.html)\n",
    "* [Dynamic Mode Decomposition (DMD) of numerical and experimental data](https://doi.org/10.1017/S0022112010001217)\n",
    "* [Proper Orthogonal Decomposition (POD) MIT notes](http://web.mit.edu/6.242/www/images/lec6_6242_2004.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "980e4642-b64a-4fa0-a192-e8aada531472",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "<div style=\"background-color: #e6ccff; padding: 10px;\">\n",
    "\n",
    "<h1> Machine Learning Theory </h1>\n",
    "\n",
    "# Modal Decomposition\n",
    "\n",
    "## The problem\n",
    "\n",
    "Flow feature extraction in fluid mechanics datasets involves identifying and characterizing significant patterns and structures within fluid flow data. This process is helpful for understanding complex flow behaviors, such as turbulence, vortex dynamics, and boundary layer interactions. By extracting these features, researchers can gain insights into the underlying physics of fluid flows and improve predictive models.\n",
    "\n",
    "Modal decomposition methods, such as Proper Orthogonal Decomposition (POD) and Dynamic Mode Decomposition (DMD), are powerful tools for flow feature extraction. These methods decompose complex flow fields into a set of orthogonal modes, each representing a distinct flow feature. By analyzing these modes, researchers can isolate and study specific flow phenomena, leading to a deeper understanding of fluid dynamics and more efficient data analysis.\n",
    "\n",
    "## Popular modal decomposition methods\n",
    "\n",
    "* Singular Value Decomposition (SVD): A fundamental linear algebra technique used to decompose a matrix into its singular values and vectors, often used in various modal analysis methods.\n",
    "* Proper Orthogonal Decomposition (POD): Also known as Principal Component Analysis (PCA) in statistics, POD identifies the most energetic modes in a flow field.\n",
    "* Dynamic Mode Decomposition (DMD): A method that decomposes complex systems into modes with specific temporal behaviors, useful for analyzing dynamic features in fluid flows.\n",
    "* Fourier Decomposition: Decomposes a signal into its constituent frequencies, often used for periodic or quasi-periodic flows.\n",
    "* Wavelet Decomposition: Provides a time-frequency representation of a signal, useful for analyzing transient and multi-scale phenomena in fluid flows."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06222208-44db-4289-a21b-eb6f1ac25b70",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #cce5ff; padding: 10px;\">\n",
    "\n",
    "# Python\n",
    "\n",
    "## [SciPy](https://scipy.org/)\n",
    "\n",
    "SciPy is a widely used open-source library for scientific and technical computing in Python. It builds on the NumPy array object and provides a large collection of algorithms and functions for numerical integration, optimization, signal processing, linear algebra, and more. SciPy enables users to perform complex scientific computations with ease and efficiency. With its intuitive Python interface, SciPy is accessible for beginners, yet it also offers advanced capabilities for experienced programmers. SciPy is compatible with various platforms, from personal computers to high-performance computing environments.\n",
    "\n",
    "## [PySINDy](https://github.com/dynamicslab/pysindy)\n",
    "\n",
    "PySINDy is a sparse regression package with several implementations for the Sparse Identification of Nonlinear Dynamical systems (SINDy) method introduced in Brunton et al. (2016a), including the unified optimization approach of Champion et al. (2019), SINDy with control from Brunton et al. (2016b), Trapping SINDy from Kaptanoglu et al. (2021), SINDy-PI from Kaheman et al. (2020), PDE-FIND from Rudy et al. (2017), and so on. A comprehensive literature review is given in de Silva et al. (2020) and Kaptanoglu, de Silva et al. (2021).\n",
    "\n",
    "## Further reading\n",
    "\n",
    "If you want to run this notebook locally or on a remote service:\n",
    "\n",
    "* [running Jupyter notebooks](https://jupyter.readthedocs.io/en/latest/running.html)\n",
    "* [installing the required Python environments](https://github.com/cemac/LIFD_ENV_ML_NOTEBOOKS/blob/main/howtorun.md)\n",
    "* [running the Jupyter notebooks locally](https://github.com/cemac/LIFD_ENV_ML_NOTEBOOKS/blob/main/jupyter_notebooks.md)\n",
    "\n",
    "</div>\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a299542-e3b2-4f6c-898d-311a1614b0c5",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #ffffcc; padding: 10px;\">\n",
    "    \n",
    "<h1> Requirements </h1>\n",
    "\n",
    "This notebook should run with the following requirements satisfied.\n",
    "\n",
    "<h2> Python Packages: </h2>\n",
    "\n",
    "* numpy\n",
    "* scipy\n",
    "* matplotlib\n",
    "* notebook\n",
    "* pysindy\n",
    "* scikit-learn\n",
    "\n",
    "<h2> Data Requirements</h2>\n",
    "\n",
    "Required data from the fluid dynamics simulations are already included in the repository as `.npz` files.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7ef9c77-4b31-451a-a388-1cc87aa111e7",
   "metadata": {},
   "source": [
    "**Contents:**\n",
    "\n",
    "1. [Overview and machine-learning theory](#Overview)\n",
    "2. [Singular Value Decomposition (SVD)](#Part-1:-SVD)\n",
    "3. [Proper Orthogonal Decomposition (POD)](#Part-2:-POD)\n",
    "4. [Dynamic Mode Decomposition (DMD)](#Part-3:-DMD)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd3e0a9a-0ab0-4952-8637-c09703fcb36d",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #cce5ff; padding: 10px;\">\n",
    "\n",
    "## Import modules\n",
    "\n",
    "First we will import all the modules needed during this tutorial.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dbe302a",
   "metadata": {},
   "source": [
    "### Note for Colab users\n",
    "\n",
    "If you are using Google Colab to run this notebook, you will need to download an additional module now by uncommenting and running the next code cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b2ffcdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !wget https://raw.githubusercontent.com/cemac/LIFD_ModalDecomposition/refs/heads/main/helper_functions.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e56fa41",
   "metadata": {},
   "source": [
    "Let's import all the libraries we need. This may take a few seconds, depending on the speed of your filesystem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c21769fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_sample_image\n",
    "from helper_functions import plot_cylinder_data\n",
    "from helper_functions import download_data\n",
    "import matplotlib.animation\n",
    "from IPython.display import HTML"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70558abb",
   "metadata": {},
   "source": [
    "## Part 1: SVD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19f4f9b9",
   "metadata": {},
   "source": [
    "Let's start by reviewing the Singular Value Decomposition (SVD). The SVD is a powerful linear algebra technique that decomposes a matrix $\\mathbf{A}$ as $\\mathbf{A}=\\mathbf{U}\\boldsymbol{\\Sigma}\\mathbf{V}^T$, where $T$ denotes the matrix transpose. Note that we have restricted ourselves to real matrices for this notebook, but everything carries over to the case of complex matrices where transposes become Hermitian transposes. The column vectors of $\\mathbf{U}$ and $\\mathbf{V}$ are known as the left and right singular vectors, respectively. Furthermore, the matrix $\\boldsymbol{\\Sigma}$ is diagonal with positive real entries. Denoting these column vectors as $\\mathbf{u}_j$ and $\\mathbf{v}_j$, we can also write the singular triplet $(\\mathbf{u}_j, \\mathbf{v}_j, \\sigma_j)$, where $\\sigma_j$ is $\\boldsymbol{\\Sigma}_{jj}$. The matrices $\\mathbf{U}$ and $\\mathbf{V}$ are unitary, meaning that $\\{\\mathbf{u}_j\\}_{j=0}$ and $\\{\\mathbf{v}_j\\}_{j=0}$ form orthogonal bases.\n",
    "\n",
    "By definition we then have that $\\mathbf{A}\\mathbf{v}_j=\\sigma_j\\mathbf{u}_j$, showing that the action of $\\mathbf{A}$ on any vector can be approximated well by the sum of a handful of vectors $\\mathbf{u}_j$ provided the singular values decay quickly. In other words, if we have the ordering $\\sigma_0>\\sigma_1>...$, then if $\\sigma_0\\gg\\sigma_1\\gg...$ the SVD can be used to create a low-rank approximation to $\\mathbf{A}$.\n",
    "\n",
    "To illustrate this, let's consider compressing an image.\n",
    "\n",
    "We first load an image of a flower, and rescale the integer red green blue (RGB) data to be floats between zero and one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90ce371c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a flower image, and rescale the RGB channels to lie within [0, 1]\n",
    "flower = np.float32(load_sample_image('flower.jpg')/255)\n",
    "channels = ['red', 'green', 'blue']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32cf15a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_image(image: np.ndarray, ax=None, title=None):\n",
    "    \"\"\"\n",
    "    Plots an image from RGB data.\n",
    "    \n",
    "    Parameters\n",
    "    ----------  \n",
    "    image: array with shape (number of pixels in y-direction,\n",
    "                             number of pixels in x-direction,\n",
    "                             channels).\n",
    "    ax: axis to plot the image in.\n",
    "    title: title for the plot.\n",
    "    \"\"\"\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots(1)\n",
    "    ax.imshow(image)\n",
    "    ax.axis('off')\n",
    "    ax.set_title(title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4845a7e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_image(flower, title='Original image')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f70c5ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Shape of our image:', flower.shape)\n",
    "print('Memory of flower: %.2f MB ' % (np.prod(flower.shape)*32/1024/1024))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bf4a576",
   "metadata": {},
   "source": [
    "This image takes $427\\times640\\times3\\times\\textrm{size of  float}\\approx 25 ~\\mathrm{MB}$. Let's see if we can compress the image by retaining a low-rank approximation where only some of the singular values are kept."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebfbf108",
   "metadata": {},
   "outputs": [],
   "source": [
    "channels = ['red', 'green', 'blue']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32c083b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_reduced(image, rank=None):\n",
    "    \"\"\"\n",
    "    Return a low-rank approximation for an image.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    image: array with shape (number of pixels in y-direction,\n",
    "                             number of pixels in x-direction,\n",
    "                             number of channels).\n",
    "    rank: How many dominant singular values to keep (default: all of them).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    reduced_image: A dictionary with size, red, green, and blue keys. The\n",
    "                   entry of size contains the shape of the image.\n",
    "                   Each channel (red, green, blue) entry is a tuple \n",
    "                   (U, Sigma, VH), such that our low rank approximation \n",
    "                   for that channel is U@Sigma@VH.\n",
    "    \"\"\"\n",
    "    reduced_image = {'size': image.shape}\n",
    "    if rank == None:\n",
    "        rank = np.min(image.shape[:2]) - 1\n",
    "    # Loop over RGB channels\n",
    "    for i, channel in enumerate(channels):\n",
    "        U, S, VH = sp.linalg.svds(flower[:, :, i], k=rank)\n",
    "        reduced_image[channel] = (U, S, VH)\n",
    "    return reduced_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2c0969c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reconstruct_image(low_rank_image):\n",
    "    \"\"\"\n",
    "    Reconstructs the image from the low rank approximation.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    low_rank_image: Low rank approximation obtained from image_reduced.\n",
    "    \"\"\"\n",
    "    # Reconstruct image\n",
    "    reconstructed_image = np.empty(shape=low_rank_image['size'], dtype=np.float32)\n",
    "    for i, channel in enumerate(channels):\n",
    "        (U, S, VH) = low_rank_image[channel]\n",
    "        reconstructed_image[:, :, i] = U @ np.diag(S) @ VH\n",
    "    return reconstructed_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3333368d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ranks = [1, 10, 40, 100]\n",
    "reduced_images = []\n",
    "for rank in ranks:\n",
    "    reduced_images.append(image_reduced(flower, rank=rank))\n",
    "fig, ax = plt.subplots(2, 2)\n",
    "for i, rank in enumerate(ranks):\n",
    "    image = reconstruct_image(reduced_images[i])\n",
    "    plot_image(image, ax=ax.flatten()[i], title=r'Rank=%d' % rank)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf3d72e3",
   "metadata": {},
   "source": [
    "From the above images we see that increasing the rank improves our low-rank approximation. Recall, that the full rank of the original image is 427. Despite this, even a rank 40 approximation is pretty good - albeit with some artifacts. At rank 100 the image is indistinguishable (at least to me). The memory requirement for a rank-100 approximation is 9.78 MB, significantly smaller than the original 25.02 MB image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdd7f7cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def memory_of_approximation(r):\n",
    "    \"\"\"\n",
    "    Returns the memory requirement for a rank-r approximation.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    r: rank of approximation.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    mem: Memory in MB needed to store the approximation.\n",
    "    \"\"\"\n",
    "    # Have r floats, r vectors of size 427, r vectors of size 640 and 3 channels\n",
    "    mem = 3*((r*427) + (r*640) + r)*32/1024/1024\n",
    "    return mem\n",
    "\n",
    "print('Memory of rank 100 approximation %.2f MB ' % (memory_of_approximation(100)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "025839fb",
   "metadata": {},
   "source": [
    "A more systematic way, than looking by eye, to gauge what rank is needed is to look at the singular values. We can do this by obtaining a full-rank approximation, and then adding up the singular values for each channel and normalising."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b668a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "flower_rank_full = image_reduced(flower)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c66269e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's look at the singular values\n",
    "svd_sum = 0\n",
    "for channel in channels:\n",
    "    (U, S, VH) = flower_rank_full[channel]\n",
    "    plt.semilogy(S[::-1]/np.max(S), color=channel)\n",
    "    svd_sum += S[::-1]\n",
    "# Normalise svd_sum\n",
    "svd_sum /= np.max(svd_sum)\n",
    "plt.semilogy(svd_sum, color='black')\n",
    "plt.grid(which='both')\n",
    "plt.xlabel(r'$j$')\n",
    "plt.ylabel(r'$\\sigma_j/\\sigma_0$')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9124147c",
   "metadata": {},
   "outputs": [],
   "source": [
    "rank_99 = np.argmax(svd_sum < 0.01)\n",
    "print(rank_99)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b44dd46",
   "metadata": {},
   "source": [
    "We see that the singular values drop off quickly, indicating that a low-rank approximation will be possible. A good rule of thumb is to set the rank such that the singular values have dropped to below 99% of the their maximum value. For the flower this obtained at 65."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d1452ca",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "Explore low-rank approximations for another image, e.g. `china.jpg`, in the scikit-image library."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a013c23d",
   "metadata": {},
   "source": [
    "## Part 2: POD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "814804bc",
   "metadata": {},
   "source": [
    "So far we've seen that the SVD is able to capture the essence of a complicated dataset by reducing it to a low-rank approximation. Let's now consider the application of the SVD to fluid mechanics datasets. To this end, let's consider the classic example of flow past a cylinder (at Reynolds number 100). The dataset is stored on Hugging Face and can be downloaded with the `download_data` helper function. First, let's examine the data. The data is stored in *.npz* format and contains a number of numpy arrays\n",
    "1. The velocity in the $x$-direction 'u' : shape (number of snapshots, $x$-resolution, $y$-resolution)\n",
    "2. The velocity in the $y$-direction 'v' : shape (number of snapshots, $x$-resolution, $y$-resolution)\n",
    "3. The $x$-coordinates of the 'u' and 'v' data data 'xu' and 'xv', respectively.\n",
    "4. The $y$-coordinates of the 'u' and 'v' data data 'yu' and 'yv', respectively.\n",
    "5. An array of times 't' at which the velocity snapshots are sampled.\n",
    "\n",
    "Also provided is the lift force on the cylinder at every timestep in the simulation (rather than the coarser time-resolution of the snapshots).\n",
    "\n",
    "6. The lift data 'lift'\n",
    "7. The times corresponding to the lift_data 'lift_time'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a203a526",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "download_data()\n",
    "data = np.load('data/cylinder_flow_data.npz')\n",
    "\n",
    "xu = data['xu']\n",
    "yu = data['yu']\n",
    "u = data['u']\n",
    "\n",
    "xv = data['xv']\n",
    "yv = data['yv']\n",
    "v = data['v']\n",
    "t = data['t']\n",
    "lift  = data['lift']\n",
    "lift_time  = data['lift_time']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a3a6b0c-b3e5-49e2-adad-f68f2155134b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(lift_time[1:], lift[1:])\n",
    "plt.xlabel(r'$t$')\n",
    "plt.ylabel(r'Lift')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8157729-339d-41fc-8291-d6445182fb4d",
   "metadata": {},
   "source": [
    "In a separate file, mass matrices are provided for the cylinder data. As the data is provided on an non-equispaced mesh, the mass matrices allow for a quantity resembling the kinetic energy to be constructed, i.e.\n",
    "$$\n",
    "\\textit{KE}=\\frac{1}{2}\\iint u\\cdot u + v\\cdot v\\;\\textrm{d}V\\approx\\frac{1}{2}\\left(\\mathbf{u}^T\\mathbf{W}_u\\mathbf{u} + \\mathbf{v}^T\\mathbf{W}_v\\mathbf{v} \\right).\n",
    "$$\n",
    " We illustrate their use by constructing a kinetic energy timeseries for the flow data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8686defc-fa05-4ba2-bf4f-761ffda703d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the mass matrices\n",
    "mass_u = sp.load_npz('data/cylinder_mass_u.npz')\n",
    "mass_v = sp.load_npz('data/cylinder_mass_v.npz')\n",
    "number_of_snapshots = u.shape[0]\n",
    "# Construct the kinetic energy\n",
    "KE = []\n",
    "for (u_snapshot, v_snapshot) in zip(u.reshape(number_of_snapshots, -1), v.reshape(number_of_snapshots, -1)):\n",
    "    KE.append(0.5*np.vdot(u_snapshot, mass_u@u_snapshot) + 0.5*np.vdot(v_snapshot, mass_v@v_snapshot))\n",
    "KE = np.array(KE)\n",
    "\n",
    "plt.plot(t, KE)\n",
    "plt.xlabel(r'$t$')\n",
    "plt.ylabel(r'Kinetic energy')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cb314e3",
   "metadata": {},
   "source": [
    "Based on the above images, we can see that the flow evolves to a periodic state which starts at around $t=200$. This is the famus Von-Karnan vortex street. For this notebook let's truncate our flow data to this periodic flow (denoted with an LC for limit-cycle)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17085096",
   "metadata": {},
   "outputs": [],
   "source": [
    "t_start = np.argmin(np.abs(t-300))\n",
    "u_LC = u[t_start:]\n",
    "v_LC = v[t_start:]\n",
    "t_LC = t[t_start:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58bccdbc-a678-4f89-bad6-c12b617e76da",
   "metadata": {},
   "source": [
    "Using our helper function for plotting the cylinder data, we can now make an animation of the flow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9445cce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "fig, ax = plt.subplots()\n",
    "# Set up plot\n",
    "mesh = plot_cylinder_data(xu, yu, u_LC[0], fig_ax=(fig, ax))\n",
    "def animate(i):\n",
    "    mesh.set_array(u_LC[i])\n",
    "    return mesh,\n",
    "    \n",
    "num_frames = 120\n",
    "anim = matplotlib.animation.FuncAnimation(fig, animate, frames=num_frames)\n",
    "HTML(anim.to_jshtml())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f74e7cb",
   "metadata": {},
   "source": [
    "By averaging, we can also construct the mean flow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0b1ccb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "u_bar = np.mean(u_LC, axis=0)\n",
    "v_bar = np.mean(v_LC, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85e151d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_cylinder_data(xu, yu, u_bar, fig_ax=None, cmap=None)\n",
    "plot_cylinder_data(xv, yv, v_bar, fig_ax=None, cmap=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32d66def-7b38-45d8-96c3-274d14bf1c4c",
   "metadata": {},
   "source": [
    "By examining the flow-field, it is evident that there is a lot of structure. This visual inspection suggests that the flow is low-rank, i.e. it can be approximated well by only a few modes. This low-rank approximation can be achieved by performing a proper orthogonal decomposition. In order to do this, we proceed as follows. Firstly, we separate the unsteady dynamics from the mean-flow by computing\n",
    "$\\mathbf{q}(t, \\mathbf{x})=\\bar{\\mathbf{q}}(\\mathbf{x}) + \\mathbf{q}'(t, \\mathbf{x})$. Note that, the mean-flow does not depend on time. However, the fluctuations do. We now seek a decomposition of the unsteady dynamics\n",
    "$$\n",
    "\\mathbf{q}' = \\sum_{i=0}^{r-1} a_i(t) \\boldsymbol{\\phi}_i(\\mathbf{x}),\n",
    "$$\n",
    "where we have assumed a separation between space and time. By further demanding that our spatial modes $\\boldsymbol{\\phi}_i$ are orthogonal (in some sense we will define later), this decomposition means that the unsteady-dynamics are being represented using a rank-$r$ approximation. Time dependence is entirely through the time-dependent coefficients $a_i$. The question is then, how to choose this basis optimally? Clearly there are a lot of choices for this basis, but ideally we want the smallest basis possible which captures the key features of the flow. This suggests that we would like a basis where $a_i$ decays quickly, meaning that most of the information is contained in a few dominant modes. Proper orthogonal decomposition provides a way to achieve this.\n",
    "\n",
    "First of all, let's consider how we want our modes to be orthogonal. If they are orthononal under the $L_2$ inner-product, then \n",
    "$$\n",
    "\\mathbf{q}'\\cdot \\mathbf{q}' = \\sum_i a_i(t)^2.\n",
    "$$\n",
    "In this manner, we see that $a_i$ is analogous to energy. This may be numerically convenient, however, depending on $\\mathbf{q}$ this discrete form of energy might not correspond to a physically meaningful one - for instance, if a non-equispaced grid is used. Ideally, we would like to $a_i$ to represent a physical energy and so we really want to do weighted inner product such that\n",
    "$$\n",
    "\\mathbf{q}'\\cdot \\mathbf{W}\\mathbf{q}' \\approx \\\\\\int_\\Omega |q|^2\\;\\textrm{d}V,\n",
    "$$\n",
    "where $\\mathbf{W}$ is a suitably chosen symmetric positive definite weight matrix (see above in the notebook, for example), and $q$ is the continous quantity that $\\mathbf{q}$ discretises. Based on this, we should take $\\boldsymbol{\\phi}_i$ orthogonal with respect to the weighted inner product with weight $\\mathbf{W}$. This means that \n",
    "$$\n",
    "\\boldsymbol{\\phi}_i\\cdot\\mathbf{W}\\boldsymbol{\\phi}_j = \\delta_{ij},\n",
    "$$\n",
    "and \n",
    "$$\n",
    "\\mathbf{q}'\\cdot \\mathbf{W}\\mathbf{q}' = \\sum_i a_i(t)^2 \\approx \\\\\\int_\\Omega |q|^2\\;\\textrm{d}V,\n",
    "$$\n",
    "showing that $a_i(t)$ now gives the amount of physical energy that mode $\\phi_i$ contains.\n",
    "\n",
    "Armed with an appropriate inner product, we are ready to formulate the problem of finding the best basis. To do this we first recognise that $a_i(t)$ is time-dependent. Therefore, we should try and pick each mode to maximise the time averaged energy\n",
    "$$\n",
    "\\bar{E_i} = \\frac{1}{N}\\sum_j a_i(t_j)^2,\n",
    "$$\n",
    "where $N$ is the number of snapshots. By arranging our flow snapshots in a matrix\n",
    "$$\\mathbf{A}=\n",
    "\\begin{pmatrix}\n",
    "\\mathbf{q}'(t_0) & \\mathbf{q}'(t_1) & \\cdots & \\mathbf{q}'(t_{N-1})\n",
    "\\end{pmatrix},\n",
    "$$\n",
    "we can form $\\bar{E_i}$ with the following steps\n",
    "1. **Project the data**\n",
    "$$\n",
    "\\begin{pmatrix} \n",
    "a_i(t_0) & a_i(t_1) & \\cdots & a_i(t_{N-1})\n",
    "\\end{pmatrix} = \\phi^T_i\\mathbf{W}\\mathbf{A}\n",
    "$$\n",
    "2. **Form the quantity we wish to maximise**\n",
    "$$ \n",
    "\\bar{E_i} = \\frac{1}{N}(\\phi_i^T\\mathbf{W}\\mathbf{A})\\cdot(\\phi^T_i\\mathbf{W}\\mathbf{A}) = \\frac{1}{N}\\boldsymbol{\\phi}^T_i\\mathbf{W}\\mathbf{A}\\mathbf{A}^T\\mathbf{W}^T\\boldsymbol{\\phi}_i\n",
    "$$\n",
    "\n",
    "Clearly from step 2 there is a relation between $\\bar{E_i}$, our basis $\\{\\boldsymbol{\\phi}_i\\}$ and the snapshot matrix. As our modes have unit energy, this maximisation problem is equivalent to maximising\n",
    "$$ \n",
    "\\bar{E_i} = \\frac{1}{N}\\frac{\\boldsymbol{\\phi}^T_i\\mathbf{W}\\mathbf{A}\\mathbf{A}^T\\mathbf{W}^T\\boldsymbol{\\phi}_i}{\\boldsymbol{\\phi}^T_i\\mathbf{W}\\boldsymbol{\\phi}_i}.\n",
    "$$\n",
    "If we now write $\\tilde{\\boldsymbol{\\phi}}_i = \\mathbf{M}\\boldsymbol{\\phi}_i$ where $\\mathbf{M}$ is found such that $\\mathbf{W}=\\mathbf{M}^T\\mathbf{M}$, this problem takes the form of a *Rayleigh quotient*\n",
    "$$ \n",
    "N\\bar{E_i} = \\frac{\\tilde{\\boldsymbol{\\phi}}^T_i\\mathbf{M}\\mathbf{A}\\mathbf{A}^T\\mathbf{M}^T\\tilde{\\boldsymbol{\\phi}}_i}{\\tilde{\\boldsymbol{\\phi}}^T_i\\tilde{\\boldsymbol{\\phi}}_i}.\n",
    "$$\n",
    "Doing this allows us to immediately solve the problem. The solution is obtained by finding the eigenvalues and eigenvectors of $\\mathbf{R}=\\mathbf{M}\\mathbf{A}\\mathbf{A}^T\\mathbf{M}^T$ (which is the covariance matrix of our snapshots!). In other words, the POD decomposition finds the optimal basis by solving\n",
    "$$\n",
    "\\mathbf{R}\\tilde{\\boldsymbol{\\phi}}_i = \\lambda_i\\tilde{\\boldsymbol{\\phi}}_i,\n",
    "$$\n",
    "where $\\lambda_i=N\\bar{E_i}$. As the covariance matrix is a symmetric positive definite matrix, the eigenvalues are real and greater than zero. Hence, we can rank them $\\lambda_0>\\lambda_1>...>\\lambda_{N-1}>0$. POD works really well when there is a fast fall off in eigenvalues, as then the majority of the time-averaged energy can be explained by only a few modal structures.\n",
    "\n",
    "### Connection with the SVD\n",
    "The issue with the POD method as explained above is that the covariance matrix $\\mathbf{R}\\in\\mathbb{R}^{M\\times M}$ is a huge matrix for most fluid flow snapshots as the number of spatial degrees of freedom $M$ is usually large. To solve this problem, POD is usually performed using the method of snapshots. This method arises by using the fact that the eigenvalues of $\\mathbf{R}$ are related to the singular values of the matrices $\\mathbf{A}^T\\mathbf{M}^T$ and $\\mathbf{M}\\mathbf{A}$. Therefore, we can instead find the singular value decomposition of $\\mathbf{M}\\mathbf{A}$, whose left singular vectors are the eigenvectors of $\\mathbf{R}$. This is a much easier problem as it can be solved by finding the eigenvectors of the $N\\times N$ matrix\n",
    "$$\n",
    "\\mathbf{Q}=\\mathbf{A}^T\\mathbf{W}\\mathbf{A}.\n",
    "$$\n",
    "This is usually a much easier problem as normally the number of snapshots is significantly less than the number of degress of freedom ($N \\ll M$). As the eigenvectors $\\mathbf{v}_i$ of this matrix are the right singular vectors of $\\mathbf{M}\\mathbf{A}$, we recover the left singular vectors (the ones we want) in a final step\n",
    "$$\n",
    "\\phi_i = \\frac{1}{\\sqrt{\\lambda_i}}\\mathbf{A}\\mathbf{v}_i.\n",
    "$$\n",
    "This is the method we now implement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b73f8f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "def POD(X, weights=None, rank=10):\n",
    "    \"\"\"\n",
    "    Computes the POD using the method of snapshots.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X: List of snapshot matrices. Each element is a snapshot matrix for a physical variable.\n",
    "       Each matrix can be multidimensional, but time must be the last axis.\n",
    "    weight: List of weight matrices for weighting the snapshots. Each element conatins\n",
    "       to the weight matrix for the corresponding element of X.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    pod_modes: Matrix of pod_modes (mode_index, variable, space).\n",
    "    eigval: eigenvalues corresponding to the pod_modes.\n",
    "    pod_mode_amplitudes: Temporal amplitudes corresponding to the pod_modes\n",
    "    \"\"\"\n",
    "    # Create the snapshot matrix\n",
    "    orig_shapes = []\n",
    "    var_lengths = []\n",
    "    A = []\n",
    "    for var in X:\n",
    "        # Store the spatial shape\n",
    "        orig_shapes.append(var.shape[:-1])\n",
    "        # Store the spatial degrees of freedom\n",
    "        var_lengths.append(np.prod(orig_shapes[-1]))\n",
    "        # Flatten and concatenate variables\n",
    "        if var.ndim != 2:\n",
    "            # Must flatten spatial dimensions before SVD\n",
    "            A.append(var.reshape((var_lengths[-1], -1)))\n",
    "        else:\n",
    "            A.append(var)\n",
    "    # Convert to numpy array\n",
    "    A = np.vstack(A)\n",
    "    # Form the weight matrix\n",
    "    if weights==None:\n",
    "        W = sp.identity(A.shape[0])\n",
    "    else:\n",
    "        W = sp.block_diag(weights)\n",
    "    # Form the covariance matrix\n",
    "    Q = A.T @ W @ A\n",
    "    # Perform the eigenvalue decompositions\n",
    "    eigval, eigvec = sp.linalg.eigs(Q, k=rank)\n",
    "    # Reconstruct the POD modes\n",
    "    pod_modes = A @ eigvec @ np.diag(1/np.sqrt(eigval))\n",
    "    # Make mode_index the first dimension\n",
    "    pod_modes = pod_modes.T\n",
    "    # Project to get mode_amplitudes\n",
    "    pod_mode_amplitudes = pod_modes @ W @ A\n",
    "    # Restructure the POD modes for output\n",
    "    pod_modes_split = np.split(pod_modes, np.cumsum(var_lengths)[:-1], axis=1)\n",
    "    pod_modes = []\n",
    "    for pod_mode, orig_shape in zip(pod_modes_split, orig_shapes):\n",
    "        pod_modes.append(pod_mode.reshape((-1,) + orig_shape))\n",
    "    return eigval, pod_modes, pod_mode_amplitudes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3446e39e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create POD data by moving time to the last axis\n",
    "POD_data_u = (u_LC - u_bar).transpose(1, 2, 0)\n",
    "POD_data_v = (v_LC - v_bar).transpose(1, 2, 0)\n",
    "# Get a rank 20 POD decomposition\n",
    "eigval, pod_modes, pod_mode_amplitudes = POD([POD_data_u, POD_data_v], rank=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a841eac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the normalise eigenvalues\n",
    "plt.semilogy(eigval/eigval[0], 'x')\n",
    "plt.xlabel(r'\\lamba_i/\\lamba_0')\n",
    "plt.ylabel(r'POD mode i')\n",
    "plt.xticks(np.arange(0, 20))\n",
    "plt.grid();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48dc9d42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the first POD-mode\n",
    "plot_cylinder_data(xu, yu, pod_modes[0][0].real, fig_ax=None, cmap='bwr')\n",
    "plot_cylinder_data(xv, yv, pod_modes[1][0].real, fig_ax=None, cmap='bwr')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89439edf-27ed-4068-94b4-70bf4be2a2a7",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "Plot the higher order POD modes. Can you explain why they seem to come in pairs?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57807766",
   "metadata": {},
   "source": [
    "## Part 3: DMD\n",
    "As we have seen, POD is able to find an orthogonal basis that captures the average energy of the flow with the fewest number of modes. However, for many fluid applications this may not be the modal decomposition we want to perform. The issue is that whilst the POD modes are linearly uncorrelated, they are nonlinearly correlated. Another way to say this, is that multiple physical processes can be present in each POD mode. In the context of cylinder flow, this can be seen by looking at the time varying amplitudes of each POD mode. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6139d00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the amplitudes of the first two pairs of modes\n",
    "first_mode_amplitude = pod_mode_amplitudes[0].real\n",
    "second_mode_amplitude = pod_mode_amplitudes[2].real\n",
    "plt.plot(t_LC, first_mode_amplitude, label=r'POD mode 0')\n",
    "plt.plot(t_LC, second_mode_amplitude, label=r'POD mode 2')\n",
    "plt.xlim([t_LC[0], t_LC[0]+6])\n",
    "plt.xlabel(r'time')\n",
    "plt.ylabel(r'Mode amplitude')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce75487a",
   "metadata": {},
   "source": [
    "This plot suggests that the POD modes consist of discrete frequencies. However, performing a fast Fourier transform (FFT) reveals that this is not completely true."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe09ec80",
   "metadata": {},
   "outputs": [],
   "source": [
    "freqs = np.fft.fftshift(np.fft.fftfreq(len(first_mode_amplitude), d=t_LC[1]-t_LC[0]))\n",
    "mode_0_fft = np.fft.fftshift(np.abs(np.fft.fft(first_mode_amplitude)))\n",
    "mode_2_fft = np.fft.fftshift(np.abs(np.fft.fft(second_mode_amplitude)))\n",
    "plt.semilogy(freqs, mode_0_fft, label=r'POD mode 0')\n",
    "plt.semilogy(freqs, mode_2_fft, label=r'POD mode 2')\n",
    "plt.ylim([1e1, 1e5])\n",
    "plt.xlim([0, 1])\n",
    "plt.xlabel('Frequency (Hz)')\n",
    "plt.ylabel('FFT amplitude')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dc6ef8c",
   "metadata": {},
   "source": [
    "The frequency spectrum shows that the first mode is predominantly the vortex shedding frequency. However, although the second mode contains mostly frequency content at the first harmonic of the fundamental frequency, this mode also contains some power at the fundamental frequency, as well as further superharmonics. This is a direct consequence of requiring the modes to be orthogonal in space. Physically, we expect that the fundamental vortex shedding mode is generated via instability and saturated via nonlinearities. In this way, perhaps a more physically relevant basis would consist of modes each having a discrete frequency. For more complex fluid flows with multiple physical phenomena, separating out each frequency can result in a clearer to understand modal decomposition, with physical processes that POD modes would blend together being more easily distinguished. This modal decomposition can be achieved by using dynamic mode decomposition (DMD), which we now describe.\n",
    "\n",
    "The DMD algorithm begins with two snapshot matrices $\\mathbf{X}$ and $\\mathbf{Y}$ where\n",
    "$$\n",
    "\\mathbf{X}=\n",
    "\\begin{pmatrix}\n",
    "\\mathbf{q}(t_0) & \\mathbf{q}(t_1) & \\cdots & \\mathbf{q}(t_{N-1})\n",
    "\\end{pmatrix},\n",
    "$$\n",
    "$$\n",
    "\\mathbf{Y}=\n",
    "\\begin{pmatrix}\n",
    "\\mathbf{q}(t_1) & \\mathbf{q}(t_2) & \\cdots & \\mathbf{q}(t_{N})\n",
    "\\end{pmatrix}.\n",
    "$$\n",
    "Note, that the columns of $\\mathbf{Y}$ by construction contain the same snapshots as $\\mathbf{X}$, except they are one timestep $\\Delta t$ ahead. DMD then assumes that we can linearly approximate the snapshots in $\\mathbf{Y}$ from those in $\\mathbf{X}$ via a matrix $\\mathbf{B}$, i.e.\n",
    "$$\n",
    "\\mathbf{Y}=\\mathbf{B}\\mathbf{X}.\n",
    "$$\n",
    "We then wish to find the eigenvalues and eigenvectors of $\\mathbf{B}$, which will give our DMD basis. The difficulty with this, is that the matrix $\\mathbf{B}$ is unknown. Therefore, we first need to approximate it from the data.\n",
    "\n",
    "To this end, we realise that if we knew the pseudo-inverse of $\\mathbf{X}$ (denoted $\\mathbf{X}^+$), then we could write \n",
    "$\\mathbf{B}=\\mathbf{Y}\\mathbf{X}^+$. It turns out that the SVD is a great way to approximate $\\mathbf{X}^+$. To see this, lets take the SVD of $\\mathbf{X}$ truncated to the first $r$ largest singular values. This gives a rank $r$ reduction of $\\mathbf{X}$\n",
    "$$\n",
    "\\mathbf{X}_r=\\mathbf{U}_r\\Sigma_r\\mathbf{V}_r^T.\n",
    "$$\n",
    "By using the fact that $\\mathbf{U}_r$ and $\\mathbf{V}_r$ are unitary, we then immediately have that\n",
    "$$\n",
    "\\mathbf{I}=\\mathbf{X}_r (\\mathbf{V}_r\\Sigma_r^{-1}\\mathbf{U}_r^T),\n",
    "$$\n",
    "showing that $\\mathbf{X}_r^+=\\mathbf{V}_r\\Sigma_r^{-1}\\mathbf{U}_r^T$. This shows that we can approximate $\\mathbf{B}$ with\n",
    "$$\n",
    "\\mathbf{B}_r = \\mathbf{Y}\\mathbf{V}_r\\Sigma_r^{-1}\\mathbf{U}_r^T.\n",
    "$$\n",
    "Similar to our POD discussion, this matrix is $M\\times M$ which is too large to be numerically feasible. Therefore, instead of finding the eigenvalue decomposition of this matrix directly, we instead write\n",
    "$$\n",
    "\\tilde{\\mathbf{B}}_r= \\mathbf{U}_r^T\\mathbf{B}_r\\mathbf{U}_r = \\mathbf{U}_r^T\\mathbf{Y}\\mathbf{V}_r\\Sigma_r^{-1}.\n",
    "$$\n",
    "As $\\mathbf{U}_r$ is unitary, the first equivalencey in the above expression shows that our $r\\times r$ matrix $\\tilde{\\mathbf{B}}_r$ shares the same eigenvalues as the larger matrix $\\mathbf{B}_r$. Hence, DMD proceeds by finding the eigenvalues $\\mu_i$ and eigenvectors $\\tilde{v}_i$ of $\\tilde{\\mathbf{B}}_r$. From this, the eigenvectors of $\\mathbf{B}_r$ can be found via $v_i=\\mu_i^{-1}\\mathbf{X}\\mathbf{V}_r\\Sigma_r^{-1}\\tilde{v}_i$.  This completes the DMD algorithm.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11dfc22e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def DMD(X, rank=10):\n",
    "    \"\"\"\n",
    "    Performs DMD on snapshot data, where time is the last axis\n",
    "    and snapshots are separated by a constant time interval.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    snapshots: List of snapshot matrices. Each element is a snapshot matrix for a physical variable.\n",
    "       Each matrix can be multidimensional, but time must be the last axis.\n",
    "    rank: rank of the approximation of B\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    eigval: eigenvalues corresponding to the DMD_modes.\n",
    "    DMD_modes: Matrix of DMD_modes (mode-index, variable, space).\n",
    "    dmd_mode_amplitudes: Array of amplitudes of the dmd modes.\n",
    "    \"\"\"\n",
    "    orig_shapes = []\n",
    "    var_lengths = []\n",
    "    A = []\n",
    "    for var in X:\n",
    "        orig_shapes.append(var.shape[:-1])\n",
    "        var_lengths.append(np.prod(orig_shapes[-1]))\n",
    "        if var.ndim != 2:\n",
    "            A.append(var.reshape((var_lengths[-1], -1)))\n",
    "        else:\n",
    "            A.append(var)\n",
    "    A = np.vstack(A)\n",
    "    X = A[:, :-1]\n",
    "    Y = A[:, 1:]\n",
    "    U, S, VH = sp.linalg.svds(X, k=rank)\n",
    "    Abar = U.conj().T @ Y @ VH.conj().T @ np.diag(1/S)\n",
    "    eigval, eigvec = np.linalg.eig(Abar)\n",
    "    dmd_modes = Y @ VH.conj().T @ np.diag(1/S) @ eigvec\n",
    "    dmd_modes = dmd_modes.T\n",
    "    indices = np.argsort(np.abs(eigval.imag))\n",
    "    eigval = eigval[indices]\n",
    "    dmd_modes = dmd_modes[indices]\n",
    "    dmd_mode_amplitudes = np.linalg.lstsq(dmd_modes.T, A)[0]\n",
    "    dmd_modes_split = np.split(dmd_modes, np.cumsum(var_lengths)[:-1], axis=1)\n",
    "    dmd_modes = []\n",
    "    for dmd_mode, orig_shape in zip(dmd_modes_split, orig_shapes):\n",
    "        dmd_modes.append(dmd_mode.reshape((-1,) + orig_shape))\n",
    "    return eigval, dmd_modes, dmd_mode_amplitudes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b55cdd58",
   "metadata": {},
   "outputs": [],
   "source": [
    "mu, dmd_modes, dmd_mode_amplitudes = DMD([POD_data_u, POD_data_v], rank=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f555e9c0",
   "metadata": {},
   "source": [
    "With the DMD modes now computed, lets compare them to the POD modes. Plotting the eigenvalues of each mode immediately shows a difference. Whereas the POD mode eigenvalues represent how much total energy of the flow they capture, the DMD mode eigenvalues represents the time-dependence of the mode. As we have performed DMD on the limit cycle data, all the modes are neutral (i.e. they lie on the unit circle). This means the modes do not grow or decay. However, they oscillate at the frequency given by the imaginary part of the eigenvalue. Note, that as for a linear equation $\\dot{\\mathbf{x}}=\\mathbf{L}\\mathbf{x}$ the DMD modes are the eigenvalues of $\\exp(\\mathbf{L}\\Delta t)$, it is common to instead convert the eigenvalues $\\mu_i$ to $\\lambda_i=\\log(\\mu_i)\\Delta t$. We display both below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3a2c29b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(8, 4))\n",
    "ax0 = fig.add_subplot(121)\n",
    "ax1 = fig.add_subplot(122)\n",
    "ax = [ax0, ax1]\n",
    "theta = np.linspace(0, 2*np.pi, 100)\n",
    "ax[0].plot(mu.imag, mu.real, 'x')\n",
    "ax[0].plot(np.cos(theta), np.sin(theta))\n",
    "ax[0].set_xlim([-1, 1])\n",
    "ax[0].set_ylim([-1, 1])\n",
    "ax[0].set_aspect('equal')\n",
    "ax[0].set_xlabel(r'Imag($\\mu$)')\n",
    "ax[0].set_ylabel(r'Real($\\mu$)')\n",
    "\n",
    "l = np.log(mu)/(t_LC[1]-t_LC[0])\n",
    "ax[1].plot(l.imag, l.real, 'x')\n",
    "xL, xR = ax[1].get_xlim()\n",
    "ax[1].plot([xL, xR], [0, 0])\n",
    "ax[1].set_ylim([-0.1, 0.1])\n",
    "ax[1].set_xlim()\n",
    "ax[1].set_xlabel(r'Imag($\\lambda$)')\n",
    "ax[1].set_ylabel(r'Real($\\lambda$)')\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48f532fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the first DMD-mode\n",
    "plot_cylinder_data(xu, yu, dmd_modes[0][0].imag, fig_ax=None, cmap='bwr')\n",
    "plot_cylinder_data(xv, yv, dmd_modes[1][0].real, fig_ax=None, cmap='bwr')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "120e4ed0",
   "metadata": {},
   "source": [
    "Although in this example the DMD modes and POD modes are very similar, a difference can be seen if we examine the amplitudes of the data projected onto the DMD modes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc7191ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the amplitudes of the first two pairs of modes\n",
    "first_mode_amplitude = dmd_mode_amplitudes[0].real\n",
    "second_mode_amplitude = dmd_mode_amplitudes[2].real\n",
    "plt.plot(t_LC, first_mode_amplitude, label=r'DMD mode 0')\n",
    "plt.plot(t_LC, second_mode_amplitude, label=r'DMD mode 2')\n",
    "plt.xlim([t_LC[0], t_LC[0]+6])\n",
    "plt.xlabel(r'time')\n",
    "plt.ylabel(r'Mode amplitude')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "132c71ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "freqs = np.fft.fftshift(np.fft.fftfreq(len(first_mode_amplitude), d=t_LC[1]-t_LC[0]))\n",
    "mode_0_fft = np.fft.fftshift(np.abs(np.fft.fft(first_mode_amplitude)))\n",
    "mode_2_fft = np.fft.fftshift(np.abs(np.fft.fft(second_mode_amplitude)))\n",
    "plt.semilogy(freqs, mode_0_fft, label=r'DMD mode 0')\n",
    "plt.semilogy(freqs, mode_2_fft, label=r'DMD mode 2')\n",
    "plt.ylim([1e0, 1e5])\n",
    "plt.xlim([0, 1])\n",
    "plt.xlabel('Frequency (Hz)')\n",
    "plt.ylabel('FFT amplitude')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4517dbd7",
   "metadata": {},
   "source": [
    "We see that each DMD mode consists of a single frequency. For flows that are strongly dominated by two or more physical phenomena, the ability of DMD to separate modes based on growth rates and frequencies can be hugely beneficial.\n",
    "\n",
    "Finally, we note that for periodic data, DMD is equivalent to performing an FFT, albeit a rather slow one. The real advantage of DMD arises when applied to more complex fluid data, where DMD can identify growth rates and damping rates.\n",
    "\n",
    "### Exercise\n",
    "Perform DMD on the growing part of the cylinder flow data. Can you determine the growth rate of the instability?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3ff5149-13ae-4153-8e77-51506b948f6c",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "   de Silva, Brian M., Kathleen Champion, Markus Quade,\n",
    "   Jean-Christophe Loiseau, J. Nathan Kutz, and Steven L. Brunton.\n",
    "   *PySINDy: a Python package for the sparse identification of\n",
    "   nonlinear dynamics from data.* arXiv preprint arXiv:2004.08424 (2020)\n",
    "   [arXiv](https://arxiv.org/abs/2004.08424)\n",
    "\n",
    "   Kaptanoglu, Alan A., Brian M. de Silva, Urban Fasel, Kadierdan Kaheman, Andy J. Goldschmidt\n",
    "   Jared L. Callaham, Charles B. Delahunt, Zachary G. Nicolaou, Kathleen Champion,\n",
    "   Jean-Christophe Loiseau, J. Nathan Kutz, and Steven L. Brunton.\n",
    "   *PySINDy: A comprehensive Python package for robust sparse system identification.*\n",
    "   arXiv preprint arXiv:2111.08481 (2021).\n",
    "   [arXiv](https://arxiv.org/abs/2111.08481)\n",
    "\n",
    "   Brunton, Steven L., Joshua L. Proctor, and J. Nathan Kutz.\n",
    "   *Discovering governing equations from data by sparse identification\n",
    "   of nonlinear dynamical systems.* Proceedings of the National\n",
    "   Academy of Sciences 113.15 (2016): 3932-3937.\n",
    "   [DOI](http://dx.doi.org/10.1073/pnas.1517384113)\n",
    "\n",
    "   Champion, K., Zheng, P., Aravkin, A. Y., Brunton, S. L., & Kutz, J. N. (2020).\n",
    "   *A unified sparse optimization framework to learn parsimonious physics-informed\n",
    "   models from data.* IEEE Access, 8, 169259-169271.\n",
    "   [DOI](https://doi.org/10.1109/ACCESS.2020.3023625)\n",
    "\n",
    "   Brunton, Steven L., Joshua L. Proctor, and J. Nathan Kutz.\n",
    "   *Sparse identification of nonlinear dynamics with control (SINDYc).*\n",
    "   IFAC-PapersOnLine 49.18 (2016): 710-715.\n",
    "   [DOI](https://doi.org/10.1016/j.ifacol.2016.10.249)\n",
    "\n",
    "   Kaheman, K., Kutz, J. N., & Brunton, S. L. (2020).\n",
    "   *SINDy-PI: a robust algorithm for parallel implicit sparse identification\n",
    "   of nonlinear dynamics.* Proceedings of the Royal Society A, 476(2242), 20200279.\n",
    "   [DOI](https://doi.org/10.1098/rspa.2020.0279)\n",
    "\n",
    "   Kaptanoglu, A. A., Callaham, J. L., Aravkin, A., Hansen, C. J., & Brunton, S. L. (2021).\n",
    "   *Promoting global stability in data-driven models of quadratic nonlinear dynamics.*\n",
    "   Physical Review Fluids, 6(9), 094401.\n",
    "   [DOI](https://doi.org/10.1103/PhysRevFluids.6.094401)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86e92706-42b5-408c-97c9-a5a13eda80cd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "MD",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
